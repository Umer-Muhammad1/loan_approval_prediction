{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "color_pal = sns.color_palette()\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"raw_loan_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58645 entries, 0 to 58644\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   id                          58645 non-null  int64  \n",
      " 1   person_age                  58645 non-null  int64  \n",
      " 2   person_income               58645 non-null  int64  \n",
      " 3   person_home_ownership       58645 non-null  object \n",
      " 4   person_emp_length           58645 non-null  float64\n",
      " 5   loan_intent                 58645 non-null  object \n",
      " 6   loan_grade                  58645 non-null  object \n",
      " 7   loan_amnt                   58645 non-null  int64  \n",
      " 8   loan_int_rate               58645 non-null  float64\n",
      " 9   loan_percent_income         58645 non-null  float64\n",
      " 10  cb_person_default_on_file   58645 non-null  object \n",
      " 11  cb_person_cred_hist_length  58645 non-null  int64  \n",
      " 12  loan_status                 58645 non-null  int64  \n",
      "dtypes: float64(3), int64(6), object(4)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_status\n",
       "0    50295\n",
       "1     8350\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#df = df.iloc[:, 1:] #dropping the id column it has nothing to do \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"train.csv\")\n",
    "#df = df.iloc[:, 1:] #dropping the id column it has nothing to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix= df.select_dtypes(include=[np.number]).corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "corr_matrix= sns.heatmap(correlation_matrix , annot=True , cmap= 'coolwarm' , fmt='.3f' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df1['person_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a lot of outliers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO this is debatable because we are removing any person with age greater than 40!\n",
    "\n",
    "Q1 = df1['person_age'].quantile(0.25)\n",
    "Q3 = df1['person_age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print('Q1:',Q1)\n",
    "print('Q3:',Q3)\n",
    "print('IQR:',IQR)\n",
    "\n",
    "# Define lower and upper bounds\n",
    "lower_bound_age = Q1 - 1.5 * IQR\n",
    "upper_bound_age = Q3 + 1.5 * IQR\n",
    "print('lower bound _age:', lower_bound_age)\n",
    "print('upper bound _age:', upper_bound_age)\n",
    "\n",
    "# Find outliers\n",
    "outliers = df1[(df1['person_age'] < lower_bound_age) | (df1['person_age'] > upper_bound_age)]\n",
    "print('number of outliers wrt age:',len(outliers))\n",
    "df2= df1.copy()\n",
    "df2 = df1[~((df1['person_age'] < lower_bound_age) | (df1['person_age'] > upper_bound_age))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df2.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Define grid size\n",
    "n_features = len(numerical_columns)\n",
    "n_cols = 3  # Number of columns in the grid\n",
    "n_rows = 3  # Calculate rows needed\n",
    "\n",
    "# Create the grid of subplots\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, n_rows * 5))\n",
    "\n",
    "# Flatten axes for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate boxplots for each numerical column\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    sns.boxplot(x=df2[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot for {col}')\n",
    "\n",
    "# Turn off unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the IQR and remove outliers\n",
    "for col in numerical_columns:\n",
    "    Q1 = df2[col].quantile(0.25)  # First quartile\n",
    "    Q3 = df2[col].quantile(0.75)  # Third quartile\n",
    "    IQR = Q3 - Q1                 # Interquartile range\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR  # Lower bound\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Upper bound\n",
    "    \n",
    "    # Filter out the outliers\n",
    "    df2 = df2[(df2[col] >= lower_bound) & (df2[col] <= upper_bound)]\n",
    "\n",
    "# Plot boxplots again to confirm removal of outliers\n",
    "n_features = len(numerical_columns)\n",
    "n_cols = 3  # Number of columns in the grid\n",
    "n_rows = 3  # Calculate rows needed\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, n_rows * 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    sns.boxplot(x=df2[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot for {col}')\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.hist(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df2.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# Display duplicate rows\n",
    "if num_duplicates > 0:\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(df2[duplicates])\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_importance(colnames_weight):\n",
    "    \n",
    "    fig,ax =plt.subplots()\n",
    "    \n",
    "    pd.Series(colnames_weight).nlargest(\n",
    "        len(colnames_weight)).plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Feature_Importance')\n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Importance')\n",
    "    fig.tight_layout()\n",
    "    img=plt.gcf()\n",
    "    plt.close()\n",
    "    \n",
    "    return img , colnames_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df3, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_encoded.drop('loan_status', axis=1)\n",
    "y= df_encoded['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train , y_test= train_test_split(X , y , test_size=0.1 , random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicate rows from the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame to remove duplicates from.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    # Remove duplicates and return the cleaned DataFrame\n",
    "    return data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd=remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(rd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(columns=['id','loan_status'])\n",
    "y=df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='loan_status', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded=pd.get_dummies(x)\n",
    "test_encoded=pd.get_dummies(test.drop(columns=['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_loan_status = train.corr(numeric_only=True)['loan_status'].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# 상관관계 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pd.DataFrame(correlation_with_loan_status), annot=True, cmap='coolwarm', fmt=\".3f\")\n",
    "plt.title('Correlation with Loan Status')\n",
    "plt.show()\n",
    "\n",
    "# 상관관계 출력\n",
    "print(correlation_with_loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features = train.columns[1:11]\n",
    "\n",
    "# 숫자형 변수만 선택\n",
    "numeric_features = train[top_10_features].select_dtypes(include=np.number)\n",
    "\n",
    "# Heatmap으로 상관관계 시각화\n",
    "correlation_matrix = numeric_features.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Top 10 Features (excluding ID)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Age group distribution (나이)\n",
    "train['age_group'] = (train['person_age'] // 10) * 10\n",
    "sns.countplot(x='age_group', data=train, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribution of Age Groups')\n",
    "axes[0, 0].set_xlabel('Age Group (10-year intervals)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Income distribution (연간 소득)\n",
    "income_bins = [0, 50000, 100000, 150000, 200000, float('inf')]\n",
    "income_labels = ['0-50K', '50K-100K', '100K-150K', '150K-200K', '200K+']\n",
    "train['income_category'] = pd.cut(train['person_income'], bins=income_bins, labels=income_labels, right=False)\n",
    "sns.countplot(x='income_category', data=train, order=income_labels, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Distribution of Person Income ')\n",
    "axes[0, 1].set_xlabel('Income')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Employment length distribution (근속 연수)\n",
    "emp_length_bins = [-1, 0, 5, 10, 15, 20, float('inf')]\n",
    "emp_length_labels = ['<1 year', '1-5 years', '6-10 years', '11-15 years', '16-20 years', '20+ years']\n",
    "train['emp_length_category'] = pd.cut(train['person_emp_length'], bins=emp_length_bins, labels=emp_length_labels, right=False)\n",
    "sns.countplot(x='emp_length_category', data=train, order=emp_length_labels, ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Distribution of Employment Length ')\n",
    "axes[0, 2].set_xlabel('Employment Length')\n",
    "axes[0, 2].set_ylabel('Count')\n",
    "axes[0, 2].set_xticklabels(emp_length_labels, rotation=45)\n",
    "\n",
    "# Loan amount distribution (대출 금액)\n",
    "loan_amnt_bins = [0, 5000, 10000, 15000, 20000, 25000, float('inf')]\n",
    "loan_amnt_labels = ['0-5K', '5K-10K', '10K-15K', '15K-20K', '20K-25K', '25K+']\n",
    "train['loan_amnt_category'] = pd.cut(train['loan_amnt'], bins=loan_amnt_bins, labels=loan_amnt_labels, right=False)\n",
    "sns.countplot(x='loan_amnt_category', data=train, order=loan_amnt_labels, ax=axes[1, 0])\n",
    "axes[0, 3].set_title('Distribution of Loan Amount ')\n",
    "axes[0, 3].set_xlabel('Loan Amount Category')\n",
    "axes[0, 3].set_ylabel('Count')\n",
    "\n",
    "# cb_person_cred_hist_length (신용 유지 기간[년])\n",
    "sns.histplot(train['cb_person_cred_hist_length'], bins=20, ax=axes[0, 3])\n",
    "axes[1, 0].set_title('Distribution of cb_person_cred_hist_length')\n",
    "axes[1, 0].set_xlabel('cb_person_cred_hist_length (Years)')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "\n",
    "# Loan interest rate distribution (대출 이자율)\n",
    "sns.histplot(train['loan_int_rate'], bins=20, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Distribution of Loan Interest Rate')\n",
    "axes[1, 1].set_xlabel('Loan Interest Rate')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "# Loan percent of income distribution (소득 대비 대출 비율)\n",
    "sns.histplot(train['loan_percent_income'], bins=20, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Distribution of Loan Amount Percentage of Income')\n",
    "axes[1, 2].set_xlabel('Loan Amount Percentage of Income')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "\n",
    "# Hide the last subplot (if unused)\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "categorical_cols = train.select_dtypes(include='object').columns\n",
    "\n",
    "num_plots = len(categorical_cols)\n",
    "rows = (num_plots + 3) // 4\n",
    "cols = min(num_plots, 4)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, rows * 4))\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "  plt.subplot(rows, cols, i + 1)\n",
    "  sns.countplot(x=train[col])\n",
    "  plt.title(f'Distribution of {col}')\n",
    "  plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded=test_encoded.reindex(columns=X_encoded.columns,fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_encoded,y,test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from joblib import Parallel, delayed\n",
    "#from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "#\n",
    "## 데이터 스케일링\n",
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "#\n",
    "## 모델 정의 수정\n",
    "#models = {\n",
    "#    'Logistic Regression': LogisticRegression(max_iter=5000, solver='liblinear'),\n",
    "#    'Decision Tree': DecisionTreeClassifier(),\n",
    "#    'Random Forest': RandomForestClassifier(n_jobs=-1,max_depth=10,n_estimators=100),\n",
    "#    'Support Vector Classifier': SVC(probability=True),\n",
    "#    'K-Nearest Neighbors': KNeighborsClassifier(n_jobs=-1),\n",
    "#    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "#    'Xgboost Classifier': XGBClassifier(n_jobs=-1, eval_metric='auc',max_depth=10,n_estimators=100)\n",
    "#}\n",
    "#\n",
    "#def train_and_evaluate(name, model, X_train, y_train, X_test, y_test):\n",
    "#    model.fit(X_train, y_train)\n",
    "#    \n",
    "#    y_train_pred = model.predict(X_train)\n",
    "#    y_test_pred = model.predict(X_test)\n",
    "#    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "#    \n",
    "#    train_score = model.score(X_train, y_train)\n",
    "#    test_score = model.score(X_test, y_test)\n",
    "#    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#    auc_score = roc_auc_score(y_test, y_test_pred_proba)\n",
    "#    \n",
    "#    return {\n",
    "#        'Model': name,\n",
    "#        'Train Score': train_score,\n",
    "#        'Test Score': test_score,\n",
    "#        'Accuracy Score': accuracy,\n",
    "#        'AUC Score': auc_score\n",
    "#    }\n",
    "#\n",
    "## 병렬 처리로 모델 학습 및 평가\n",
    "#results = Parallel(n_jobs=-1)(\n",
    "#    delayed(train_and_evaluate)(\n",
    "#        name, model, X_train_scaled, y_train_smote, X_test_scaled, y_test\n",
    "#    ) for name, model in models.items()\n",
    "#)\n",
    "#\n",
    "#results_df = pd.DataFrame(results)\n",
    "#print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 8))\n",
    "#for name, model in models.items():\n",
    "#    model.fit(X_train_scaled, y_train_smote)\n",
    "#    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "#    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "#    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "#    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "#\n",
    "#plt.plot([0, 1], [0, 1], 'k--')\n",
    "#plt.xlim([0.0, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "#plt.xlabel('False Positive Rate')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "#plt.title('ROC Curves for Different Models')\n",
    "#plt.legend(loc=\"lower right\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate_xgb(model, X_train, y_train, X_test, y_test):\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "#     auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "#     return model, auc_score\n",
    "# \n",
    "# xgb = XGBClassifier(n_jobs=-1, eval_metric='auc', max_depth=10, n_estimators=100)\n",
    "# results = Parallel(n_jobs=-1)(\n",
    "#     delayed(train_and_evaluate_xgb)(\n",
    "#         xgb, X_train_smote, y_train_smote, X_test_scaled, y_test\n",
    "#     ) for _ in range(1)\n",
    "# )\n",
    "# \n",
    "# best_model, auc_score = results[0]\n",
    "# print(f\"XGBoost 모델의 AUC-ROC 점수: {auc_score:.4f}\")\n",
    "# \n",
    "# # 제출 데이터 예측\n",
    "# submission_pred = best_model.predict(test_encoded)\n",
    "# submission_df = pd.DataFrame({'id': test['id'], 'loan_status': submission_pred})\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "# sub = pd.read_csv('submission.csv')\n",
    "# sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = {\n",
    "    \"A\": \"#1f77b4\",  # Blue\n",
    "    \"B\": \"#ff7f0e\",  # Orange\n",
    "    \"C\": \"#2ca02c\",  # Green\n",
    "    \"D\": \"#d62728\",  # Red\n",
    "    \"E\": \"#9467bd\",  # Purple\n",
    "    \"F\": \"#8c564b\",  # Brown\n",
    "    \"G\": \"#e377c2\"   # Pink\n",
    "}\n",
    "fix, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.boxplot(ax=ax, data=df, x=\"loan_grade\", y=\"loan_percent_income\", palette=custom_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Define a list of colors for each category\n",
    "custom_colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"]  # Blue, Orange, Green, Red\n",
    "\n",
    "sns.countplot(ax=ax, data=df, x=\"person_home_ownership\", palette=custom_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "sns.kdeplot(data=df, x=df[df[\"loan_status\"] == 1][\"loan_amnt\"], fill=True, label=\"Default\")\n",
    "sns.kdeplot(data=df, x=df[df[\"loan_status\"] == 0][\"loan_amnt\"], fill=True, label=\"Non-Default\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores = []\n",
    "for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx, :], X_train.iloc[val_idx, :]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=10000,\n",
    "        objective=\"binary:logistic\",\n",
    "        learning_rate=0.01,\n",
    "        early_stopping_rounds=50,\n",
    "        enable_categorical=True,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "        verbose=False  # Set verbose to True to see training output\n",
    "    )\n",
    "    \n",
    "    score = model.score(X_val, y_val)  # Evaluate on validation set\n",
    "    cv_scores.append(score)\n",
    "    print(score)\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {np.mean(cv_scores):.2f} ± {np.std(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,12))\n",
    "\n",
    "fi = pd.DataFrame(data=model.feature_importances_, index=model.feature_names_in_, columns=[\"importance\"]).sort_values(\"importance\")\n",
    "fi.plot(kind=\"barh\", title=\"Feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_box  = [\n",
    "    'person_age', 'person_income', 'loan_amnt', 'loan_int_rate', 'person_emp_length','cb_person_cred_hist_length'\n",
    "]\n",
    "for boxcol in loan_box:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(x= boxcol , data = df, hue = 'loan_status', palette='viridis', showmeans = True)\n",
    "    plt.title(f'Box plot of :{boxcol}')\n",
    "    plt.ylabel('counts')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the swarmplot with customizations\n",
    "ax = sns.swarmplot(\n",
    "    data=df.sample(1000),  # Sample 1000 data points\n",
    "    y='loan_amnt', \n",
    "    x='person_emp_length', \n",
    "    hue='loan_status',\n",
    "    size=6,               # Adjust the size of the points\n",
    "    edgecolor='black',    # Add black borders\n",
    "    linewidth=0.5,        # Set the border thickness\n",
    "    alpha=0.8,            # Add transparency\n",
    "    marker='o'            # Set the marker shape\n",
    ")\n",
    "\n",
    "# Customize axis lines\n",
    "ax.spines['bottom'].set_color('blue')  # X-axis line color\n",
    "ax.spines['left'].set_color('green')   # Y-axis line color\n",
    "ax.spines['bottom'].set_linewidth(2)   # X-axis line thickness\n",
    "ax.spines['left'].set_linewidth(2)     # Y-axis line thickness\n",
    "\n",
    "# Hide the top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Tighten the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.swarmplot(data = df.sample(1000), x = 'loan_grade' ,y = 'person_age', hue = 'loan_status', palette='viridis')\n",
    "plt.title('Relation of age and grade')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.swarmplot(data = df.sample(1000), x = 'person_home_ownership' ,y = 'loan_amnt', hue = 'loan_status', palette='viridis')\n",
    "plt.title('Relation of amt and ownership')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data = df, y = 'loan_amnt' ,x = 'loan_grade',hue = 'loan_status' )\n",
    "plt.title('Relation of age and grade')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define columns for histogram plotting\n",
    "hist_plot = [\n",
    "    'person_age', 'person_income', 'loan_amnt', 'loan_int_rate',\n",
    "    'person_emp_length', 'cb_person_cred_hist_length'\n",
    "]\n",
    "\n",
    "# Set up the grid layout\n",
    "num_plots = len(hist_plot)\n",
    "rows = (num_plots + 2) // 3  # Arrange in 3 columns\n",
    "cols = 3\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 4))\n",
    "axes = axes.flatten()  # Flatten to iterate over all axes\n",
    "\n",
    "# Loop through the columns and create histograms\n",
    "for i, col in enumerate(hist_plot):\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=col,\n",
    "        hue='loan_status',\n",
    "        palette='viridis',\n",
    "        kde=True,\n",
    "        ax=axes[i]  # Specify the subplot axis\n",
    "    )\n",
    "    axes[i].set_title(f'Histogram of {col}', fontsize=12)\n",
    "    axes[i].set_xlabel(col, fontsize=10)\n",
    "    axes[i].set_ylabel('Count', fontsize=10)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def plot_box_and_kde(df, column):\n",
    "    # 创建一个图形对象，并使用 GridSpec 定义网格布局\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs = GridSpec(1, 3, figure=fig)  # 创建1行3列的网格，宽度比为1:2\n",
    "    \n",
    "    # 绘制箱型图，占据网格1/3的宽度\n",
    "    ax1 = plt.subplot(gs[0, 0])\n",
    "    sns.boxplot(y=df[column], color='salmon', ax=ax1)\n",
    "    ax1.set_title(f'Box Plot of {column}')\n",
    "    \n",
    "    # 绘制核密度图，占据网格2/3的宽度\n",
    "    ax2 = plt.subplot(gs[0, 1:])\n",
    "    sns.kdeplot(data=df, x=column, hue=\"loan_status\", common_norm=False, ax=ax2)  # 每个分组分别独立标准化\n",
    "    ax2.set_title(f'KDE Plot of {column}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 以贷款是否被批准分组，不同组分布差别较大的变量有贷款金额、贷款年利率、年收入、贷款收入比。\n",
    "# plot_box_and_kde(df_train_cleaned, 'person_age')\n",
    "# plot_box_and_kde(df_train_cleaned, 'person_emp_length')\n",
    "plot_box_and_kde(df, 'loan_amnt')\n",
    "plot_box_and_kde(df, 'loan_int_rate')\n",
    "plot_box_and_kde(df, 'person_income')\n",
    "plot_box_and_kde(df, 'loan_percent_income')\n",
    "# plot_box_and_kde(df_train_cleaned, 'cb_person_cred_hist_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"train.csv\")\n",
    "df_test= pd.read_csv('test.csv')\n",
    "df_train= df_train.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = df_train.isnull()\n",
    "\n",
    "# Count null values per column\n",
    "null_counts = df_train.isnull().sum()\n",
    "\n",
    "print(\"Null Values in DataFrame:\")\n",
    "print(len(null_values))\n",
    "\n",
    "print(\"\\nCount of Null Values in Each Column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_train.columns[1:]:\n",
    "    print(f'{item} : {df_train[item].unique()}')\n",
    "    print('-'* 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = df_train.select_dtypes(include=['object']).columns\n",
    "for items in cat_col:\n",
    "    print(f'{df_train[items].value_counts()}')\n",
    "    print('-'* 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "df_train['loan_status'].value_counts().plot.pie(autopct = '%1.1f%%', explode = [0.15, 0.15], shadow = True).set_title('status counts')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_train[df_train['person_age']<90].reset_index(drop=True)\n",
    "train_df1 = train_df[train_df['person_income']<1e6].reset_index(drop=True)\n",
    "df2 = train_df1[train_df1['person_emp_length']<60].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_file ={\n",
    "    'N' : 0,\n",
    "    'Y' : 1\n",
    "    }\n",
    "\n",
    "df2['cb_person_default_on_file'] = df2['cb_person_default_on_file'].map(on_file)\n",
    "df_test['cb_person_default_on_file'] = df_test['cb_person_default_on_file'].map(on_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df2.select_dtypes(['object']).columns\n",
    "stand_col = ['person_age', 'person_income','person_emp_length', 'loan_amnt','loan_int_rate', 'loan_percent_income','cb_person_cred_hist_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df2.drop(columns='loan_status', axis =1)\n",
    "y_train = df2['loan_status']\n",
    "x_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_concatenate(X_train: pd.DataFrame, X_test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Concatenates training and test datasets, applies one-hot encoding, \n",
    "    and splits the datasets back into training and test sets.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training feature matrix.\n",
    "        X_test (pd.DataFrame): Testing feature matrix.\n",
    "        cat_cols (list): List of categorical columns to be one-hot encoded.\n",
    "\n",
    "    Returns:\n",
    "        tuple: One-hot encoded X_train and X_test datasets.\n",
    "    \"\"\"\n",
    "    # Concatenate X_train and X_test\n",
    "    combined_data = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "    cat_cols= combined_data.select_dtypes(['object']).columns\n",
    "    # One-hot encode the categorical columns\n",
    "    onehot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    encoded_data = onehot_encoder.fit_transform(combined_data[cat_cols])\n",
    "    \n",
    "    # Convert encoded data to DataFrame\n",
    "    encoded_columns = onehot_encoder.get_feature_names_out(cat_cols)\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns)\n",
    "\n",
    "    # Drop the original categorical columns and add the encoded ones\n",
    "    combined_data_encoded = combined_data.drop(columns=cat_cols).join(encoded_df)\n",
    "    \n",
    "    # Split the data back into X_train and X_test\n",
    "    X_train_encoded = combined_data_encoded.iloc[:len(X_train), :]\n",
    "    X_test_encoded = combined_data_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "    return X_train_encoded, X_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_diff_model(x_train, y_train, x_test, model):\n",
    "    combine = pd.concat([x_train, x_test])\n",
    "    coltrans = make_column_transformer(\n",
    "        (OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "        (StandardScaler(), stand_col),\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    pipe = make_pipeline(coltrans, model)\n",
    "    cv_scor = cross_val_score(pipe, x_train, y_train, cv =10  )\n",
    "\n",
    "    print(f'For the model {model.__class__.__name__}')\n",
    "    print(f'The Cv score is : {cv_scor}')\n",
    "    print(f'Mean of the CV is : {cv_scor.mean():.4f}')\n",
    "    print(f'SD of the CV is : {cv_scor.std():.4f}')\n",
    "\n",
    "    pipe.fit(x_train, y_train)\n",
    "    ypred = pipe.predict(x_test)\n",
    "\n",
    "    return ypred\n",
    "\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    LinearSVC(),\n",
    "    XGBClassifier(n_estimators = 350),\n",
    "    RandomForestClassifier(n_estimators=150),\n",
    "    AdaBoostClassifier(learning_rate=.15,n_estimators=100),\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    GradientBoostingClassifier(n_estimators=200, learning_rate=0.15)\n",
    "]\n",
    "predictions = []\n",
    "for model in models :\n",
    "    predic = testing_diff_model(x_train, y_train, x_test, model)\n",
    "    predictions.append(predic)\n",
    "    testing_diff_model(x_train, y_train, x_test, model)\n",
    "    print('-'* 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = df_test['id']\n",
    "output = pd.DataFrame({\n",
    "    'id' : test_id,\n",
    "    'loan_status' : predic\n",
    "})\n",
    "output.to_csv('To_submmit.csv', index=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
